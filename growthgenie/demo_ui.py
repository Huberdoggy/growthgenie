import streamlit as st
from prompt_logic import generate_prompt
from persona_loader import load_persona
from explanation_builder import build_trait_explanation
from trait_modifiers import THRESHOLD, PUBLIC_SAFE_TRAITS

persona_metadata = {
    "Kyle": {
        "emoji": "ğŸ¦‘",
        "bio": "Sharp-tongued but sincere, Kyle weaves logic with humor. Heâ€™s your go-to when clarity meets curiosity."
    },
    "Renae": {
        "emoji": "ğŸ§‘â€ğŸ”¬",
        "bio": "Analytical yet warm, Renae sees nuance where others rush."
    }
}


# --- UI Header ---
st.title("ğŸ§ GrowthGenie")
st.caption("Shape the voice of your AI with personality traits. No prompt engineering required.")

# --- Prompt Input ---
user_prompt = st.text_area("ğŸ’¬ Enter your base prompt:", value="Explain how LLMs generate human-like text.")

# --- Persona Selection ---
persona_id = st.selectbox("ğŸ­ Choose a persona:", options=["Kyle", "Renae"])
# --- Persona Preview ---
meta = persona_metadata.get(persona_id)
if meta:
    st.markdown(
        f"""
        <div style="
            background-color: #2a2a2e;
            padding: 10px 16px;
            border-left: 5px solid #007acc;
            border-radius: 8px;
            margin-top: 10px;
            margin-bottom: 10px;
            font-size: 15px;
        ">
        <strong>{meta['emoji']} {persona_id}</strong><br>
        <em>{meta['bio']}</em>
        </div>
        """,
        unsafe_allow_html=True
    )


# --- Trait Slider Preview ---
st.subheader("ğŸ›ï¸ Trait Tuning (0â€“10)")
all_traits = ["witty", "curious", "empathetic", "analytical", "direct", "imaginative"] # Removed 'precise'

# --- Mode Toggle ---
external_mode = st.checkbox("External Mode (Public-safe output)", value=False)

PRESETS = {
    "Atlas Mode": {"curious": 0.9, "analytical": 0.9, "witty": 0.8},
    "Make It More Human": {"empathetic": 0.9, "witty": 0.8},
    "Brand Voice Booster": {"imaginative": 0.9, "direct": 0.8},
    "The Conversational Researcher": {"curious": 0.8, "empathetic": 0.6, "analytical": 0.5}
}

st.subheader("ğŸ§‘â€ğŸ“ Guided Mode Presets")

# Filter presets based on mode
if external_mode:
    safe_presets = ["The Conversational Researcher"]
else:
    safe_presets = list(PRESETS.keys())

# Conveniently reset all sliders to zero
if st.button("Reset Traits"):
    st.session_state["selected_preset"] = "None"
    for t in all_traits:
        st.session_state[t] = 0

# 'safe_presets' will either contain only public-mode preset, or all others
selected_preset = st.selectbox(
    "Choose a preset to auto-fill trait sliders:",
    ["None"] + safe_presets,
    key="selected_preset"
)


# Handle preset injection
if selected_preset != "None":
    # First clear all sliders
    for t in all_traits:
        st.session_state[t] = 0

    # Then apply preset values
    for t, v in PRESETS[selected_preset].items():
        st.session_state[t] = int(v * 10)  # scale to 0â€“10 slider range


trait_inputs = {}
for trait in all_traits:
    if external_mode and trait not in PUBLIC_SAFE_TRAITS:
        continue  # Skip non-public traits when in external mode
    trait_inputs[trait] = st.slider(
        trait.title(), 0, 10, 0, key=trait  # key allows programmatic control via st.session_state
    )
    
# Normalize slider values to 0.0â€“1.0
normalized_traits = {trait: value / 10 for trait, value in trait_inputs.items()}
nonzero_count = sum(1 for val in normalized_traits.values() if val > 0)

# Control generate button state -> 3 traits for 3 'wishes', so to speak...
generate_disabled = nonzero_count > 3
if generate_disabled:
    st.caption("âš ï¸ Limit: Please select no more than 3 traits per persona.")

# --- Generate Button ---
if st.button("âœ¨ Generate", disabled=generate_disabled):
    
    # Load persona for reference (could be shown later)
    persona = load_persona(persona_id)

    # Generate tuned prompt. Use persona traits unless user has adjusted sliders
    traits_to_use = normalized_traits if any(normalized_traits.values()) else None

    tuned_prompt = generate_prompt(
        user_prompt,
        persona_id,
        external_mode=external_mode,
        traits_override=traits_to_use
    )

    # --- Output Section ---
    st.markdown("#### ğŸ”® GrowthGenie Output")
    st.caption("Hereâ€™s how your tuned AI responded:")
    st.markdown(
        f"""
        <div style="
            background-color: #2a2a2e;
            padding: 12px 16px;
            border-left: 5px solid #007acc;
            border-radius: 8px;
            margin-bottom: 10px;
            max-width: 90%;
            word-wrap: break-word;
        ">
            <strong>ğŸ’¬ You asked:</strong><br><em>{user_prompt}</em>
        </div>
        """,
        unsafe_allow_html=True
    )
    
    # Split prompt from modifiers using double line break
    base, *modifiers = tuned_prompt.split("\n\n", 1)

    # We've already displayed `base` in the styled bubble.
    # Now just show the rest in the assistant's block.
    if modifiers:
        st.chat_message("assistant").write(modifiers[0])

    if external_mode:
    # Apply same public-safe gating logic for explanation
        filtered_traits = {trait: weight for trait, weight in (traits_to_use or persona.traits).items()
                           if trait in PUBLIC_SAFE_TRAITS and weight >= THRESHOLD}

        if filtered_traits:  # Prevents empty explanation
            st.markdown("---")  # Visual break
            st.caption("ğŸ§¬ Trait influence breakdown")
            explanation = build_trait_explanation(filtered_traits)
            st.markdown(f"<div style='margin-top: 10px; font-style: italic;'>{explanation}</div>", unsafe_allow_html=True)

    
